{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\vikas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\vikas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\vikas\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
    }
   ],
   "source": [
    "# importing Library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "\n",
    "import re\n",
    "import os\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA GATHERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grathering all file from directory\n",
    "\n",
    "all_files = []\n",
    "\n",
    "for path, subdirs, files in os.walk(r'C:/Users/vikas/Desktop/articles'):\n",
    "    for name in files:\n",
    "        all_files.append(os.path.join(path, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "100%|██████████| 10/10 [00:40<00:00,  4.00s/it]\n"
    }
   ],
   "source": [
    "# Loop through the all file and extracting text\n",
    "\n",
    "all_text=[]\n",
    "for f in tqdm(all_files):\n",
    "    with pdfplumber.open(f) as pdf:\n",
    "        pdf_text = []\n",
    "        pdf_text.append(f)\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text(x_tolerance=0, y_tolerance=0)\n",
    "            text = text.lower()\n",
    "            text = re.sub('†','', text)\n",
    "            text = re.sub('‡','', text)\n",
    "            text = re.sub('∗','', text)\n",
    "            text = re.sub('\\[.*?\\]', '', text)\n",
    "            text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "            text = re.sub('<.*?>+', '', text)\n",
    "            text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "            text = re.sub('\\n', '', text)\n",
    "            text = re.sub('\\w*\\d\\w*', '', text)\n",
    "            pdf_text.append(text)\n",
    "        all_text.append(pdf_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['a retrospective analysis of the fake news challengestance detection task   andreas hanselowski  avinesh pvs  benjamin schiller  felix caspelherr    debanjan chaudhuri  christian m meyer  iryna gurevychresearch training group aiphescomputer science department technische universität darmstadtsmart data analytics university of bonn darmstadtdeabstractthe  fake news challenge stage   shared task addressed a stance classiﬁcationtask as a crucial ﬁrst step towards detecting fake news to date there is no indepth analysis paperto critically discuss ’s experimental setup reproduce the results and draw  nextgeneration stance classiﬁcation methods in this paper we provide such an  for the three topperforming systems we ﬁrst ﬁnd that ’s proposed  favors the majority class which can be easily classiﬁed and thus overestimates the  discriminative power of the methods therefore we propose a new  metric yieldingna changed system ranking next we compare the features and architectures used which leadsujto a novel featurerich stacked lstm model that performs on par with the best systems but is  in predicting minority classes to understand the methods’ ability to generalize  a new dataset and perform both indomain and crossdomain experiments our qualitative  and quantitative study helps interpreting the original  scores and understand which featuresrhelp improving performance and why our new dataset and all source code used during  study are publicly available for future research  introduction   pomerleau and rao  organized the ﬁrst fake news challenge  in order  the development of ai technology to automatically detect fake news the challenge  attention in the nlp community  teams from both academia and industry participated  of the  challenge is to determine the perspective or  of a news article relative to  disagree discussgiven headline an article’s stance can either or with the headline the same  it is completely  table  shows four example documents illustrating these  detection is a crucial building block for a variety of tasks such as analyzing online  et al  sridhar et al  somasundaran and wiebe  determining the veracity  on twitter lukasik et al  derczynski et al  or understanding the argumentativevstructure of persuasive essays stab and gurevych  while stance detection has been previouslyixfocused on individual sentences or phrases the systems participating in  have to detect the stanceraof an entire document which raises many new challenges although the disagreeing article of table  leans against the headline’s claim the fourth sentence would agree to it if considered in isolationto properly learn from a scientiﬁc shared task there are typically overview and analysis papers thatcompare the architectures features and results of the participating systems to date there is howeverno such paper for  which is why we conduct a reproduction study of the top three participatingsystems our goal is to independently verify the results reported in the challenge which is an importantasset in empirical research to critically assess the experimental setup of  and to learn buildingthe work by debanjan chaudhuri has been carried out during his internship at the ubiquitous knowledge  lab  adaptive preparation of information from heterogeneous sources aiphes from  to  news  work is licensed under a creative commons attribution  international license license ']"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# preview\n",
    "\n",
    "all_text[1][1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coverting List of text to dataframe\n",
    "items = []\n",
    "for item in all_text:\n",
    "    items.append(item[1:])\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'text': [items]}) \n",
    "df = df.explode('text').reset_index(drop=True)\n",
    "\n",
    "# List of dates\n",
    "publish_date = ['27-01-2020', '13-06-2018', '08-02-2019', '08-08-2019','03-11-2019', '06-01-2020', '18-03-2020', '07-04-2020','01-03-2020', '03-11-2019']\n",
    "\n",
    "# List of title\n",
    "title = ['Sentiment Classification Based on Part-of-Speech and Self-Attention Mechanism',\n",
    "'A Retrospective Analysis of the Fake News Challenge Stance Detection Task',\n",
    "'Detecting Incongruity Between News Headline and Body Text via a Deep Hierarchical Encoder',\n",
    "'Your Stance is Exposed! Analysing Possible Factors for Stance Detection on Social Media',\n",
    "'Part of Speech Tagging for Code Switched Data',\n",
    "'Stance Detection Benchmark: How Robust Is Your Stance Detection?',\n",
    "'A Multilingual Multi-Target Dataset for Stance Detection',\n",
    "'A Few Topical Tweets are Enough for Effective Stance Detection',\n",
    "'Generating Summaries Through Unigram and Bigram: Text Summarization',\n",
    "'Incorporating Label Dependencies in Multilabel Stance Detection',]\n",
    "\n",
    "# Mergering list to dataframe\n",
    "df['publish_date'] = publish_date \n",
    "df['title'] = title\n",
    "\n",
    "# sort by date\n",
    "df['publish_date'] =pd.to_datetime(df.publish_date)\n",
    "df.sort_values(by='publish_date',ascending=False, inplace=True, )\n",
    "\n",
    "\n",
    "\n",
    "# rearrange the columns\n",
    "df = df[['publish_date', 'title','text']].reset_index(drop=True)\n",
    "df1 = df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trend Analysis\n",
    "For the trend analysis we can calculate word count of document know the distribution and extract the word frequency and keyword over the year will show the usage of words in particular year and how trend is changeing in particular feild.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaing the text\n",
    "df['clean_text'] = [\" \".join(text) for text in df['text'].values]\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    remove_stopwords = [w for w in tokenized_text if w not in stop_words]\n",
    "    combined_text = ' '.join(remove_stopwords)\n",
    "    return combined_text\n",
    "\n",
    "df['clean_text'] = df['clean_text'].apply(lambda x : text_preprocessing(x))\n",
    "\n",
    "# Fetch word count for each text\n",
    "df['word_count'] = df['clean_text'].apply(lambda x: len(str(x).split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  publish_date                                              title  \\\n0   2020-07-04  A Few Topical Tweets are Enough for Effective ...   \n1   2020-06-01  Stance Detection Benchmark: How Robust Is Your...   \n2   2020-03-18  A Multilingual Multi-Target Dataset for Stance...   \n3   2020-01-27  Sentiment Classification Based on Part-of-Spee...   \n4   2020-01-03  Generating Summaries Through Unigram and Bigra...   \n5   2019-08-08  Your Stance is Exposed! Analysing Possible Fac...   \n6   2019-08-02  Detecting Incongruity Between News Headline an...   \n7   2019-03-11      Part of Speech Tagging for Code Switched Data   \n8   2019-03-11  Incorporating Label Dependencies in Multilabel...   \n9   2018-06-13  A Retrospective Analysis of the Fake News Chal...   \n\n                                                text  \\\n0  [a few topical tweets are enough for effective...   \n1  [stance detection benchmark how robust is your...   \n2  [stancexa multilingual multitarget dataset for...   \n3  [received january   accepted january   date of...   \n4  [inte rna tiona l j ournal of information te c...   \n5  [your stance is exp ose d analysing possible f...   \n6  [detecting incongruity between news headline a...   \n7  [part of speech tagging for code switched data...   \n8  [incorporating label dependencies in multilabe...   \n9  [a retrospective analysis of the fake news cha...   \n\n                                          clean_text  word_count  \n0  topical tweets enough effective stance detecti...        3205  \n1  stance detection benchmark robust stance detec...        3663  \n2  stancexa multilingual multitarget dataset stan...        3278  \n3  received january accepted january date publica...        3896  \n4  inte rna tiona l j ournal information te chnol...        4831  \n5  stance exp ose analysing possible factors fors...        7875  \n6  detecting incongruity news headline body textv...        4193  \n7  part speech tagging code switched data fahad a...        3089  \n8  incorporating label dependencies multilabel st...        1701  \n9  retrospective analysis fake news challengestan...        4911  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>publish_date</th>\n      <th>title</th>\n      <th>text</th>\n      <th>clean_text</th>\n      <th>word_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-07-04</td>\n      <td>A Few Topical Tweets are Enough for Effective ...</td>\n      <td>[a few topical tweets are enough for effective...</td>\n      <td>topical tweets enough effective stance detecti...</td>\n      <td>3205</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-06-01</td>\n      <td>Stance Detection Benchmark: How Robust Is Your...</td>\n      <td>[stance detection benchmark how robust is your...</td>\n      <td>stance detection benchmark robust stance detec...</td>\n      <td>3663</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-03-18</td>\n      <td>A Multilingual Multi-Target Dataset for Stance...</td>\n      <td>[stancexa multilingual multitarget dataset for...</td>\n      <td>stancexa multilingual multitarget dataset stan...</td>\n      <td>3278</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-01-27</td>\n      <td>Sentiment Classification Based on Part-of-Spee...</td>\n      <td>[received january   accepted january   date of...</td>\n      <td>received january accepted january date publica...</td>\n      <td>3896</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-01-03</td>\n      <td>Generating Summaries Through Unigram and Bigra...</td>\n      <td>[inte rna tiona l j ournal of information te c...</td>\n      <td>inte rna tiona l j ournal information te chnol...</td>\n      <td>4831</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2019-08-08</td>\n      <td>Your Stance is Exposed! Analysing Possible Fac...</td>\n      <td>[your stance is exp ose d analysing possible f...</td>\n      <td>stance exp ose analysing possible factors fors...</td>\n      <td>7875</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2019-08-02</td>\n      <td>Detecting Incongruity Between News Headline an...</td>\n      <td>[detecting incongruity between news headline a...</td>\n      <td>detecting incongruity news headline body textv...</td>\n      <td>4193</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2019-03-11</td>\n      <td>Part of Speech Tagging for Code Switched Data</td>\n      <td>[part of speech tagging for code switched data...</td>\n      <td>part speech tagging code switched data fahad a...</td>\n      <td>3089</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2019-03-11</td>\n      <td>Incorporating Label Dependencies in Multilabel...</td>\n      <td>[incorporating label dependencies in multilabe...</td>\n      <td>incorporating label dependencies multilabel st...</td>\n      <td>1701</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2018-06-13</td>\n      <td>A Retrospective Analysis of the Fake News Chal...</td>\n      <td>[a retrospective analysis of the fake news cha...</td>\n      <td>retrospective analysis fake news challengestan...</td>\n      <td>4911</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0    topical tweets enough effective stance detecti...\nName: clean_text, dtype: object"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df['clean_text'][[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the vocabilary\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "word_vectorizer = TfidfVectorizer(\n",
    "    sublinear_tf=True,\n",
    "    strip_accents='unicode',\n",
    "    analyzer='word',\n",
    "    stop_words=stop_words,\n",
    "    ngram_range=(1, 2),\n",
    "    max_features=10000)\n",
    "\n",
    "X=word_vectorizer.fit_transform(df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    " \n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    " \n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "    \n",
    "    # word index and corresponding tf-idf score\n",
    "    for idx, score in sorted_items:\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    " \n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    " \n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(X)\n",
    "# get feature names\n",
    "feature_names=word_vectorizer.get_feature_names()\n",
    " \n",
    "# fetch document for which keywords needs to be extracted\n",
    "article=df['clean_text'][[6]]\n",
    " \n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(word_vectorizer.transform(article))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['body text', 'incongruent', 'incongruence', 'ip', 'ahde']"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,5)\n",
    "keyword = []\n",
    "for k in keywords:\n",
    "    keyword.append(k)\n",
    "keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword = [['expanded', 'darwish', 'retweeted', 'unsupervised classification','expanded test'], \n",
    "            ['resilience', 'spelling', 'biases', 'attacks','std datasets','related tasks','low resource'],\n",
    "            ['answers', 'french', 'switzerland', 'swiss', 'crosstarget'], \n",
    "            ['focal', 'mechanism', 'focal loss', 'conf', 'sentimental'],\n",
    "            ['summarization','phological','classif','summaries'], \n",
    "            ['body text', 'incongruent', 'incongruence', 'ip', 'ahde'],\n",
    "            ['stancedete','predetection', 'tweets', 'performance','model'], \n",
    "            ['monolingual taggers', 'madamira', 'language pairs',  'pos taggers', 'condition','arabic'], \n",
    "            ['bbc', 'mtlxld', 'mftc', 'multilabel', 'multiclass'], \n",
    "            ['headline document', 'headline', 'gaza', 'bound', 'athene']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "  publish_date                                              title  \\\n0   2020-07-04  A Few Topical Tweets are Enough for Effective ...   \n1   2020-06-01  Stance Detection Benchmark: How Robust Is Your...   \n2   2020-03-18  A Multilingual Multi-Target Dataset for Stance...   \n3   2020-01-27  Sentiment Classification Based on Part-of-Spee...   \n4   2020-01-03  Generating Summaries Through Unigram and Bigra...   \n5   2019-08-08  Your Stance is Exposed! Analysing Possible Fac...   \n6   2019-08-02  Detecting Incongruity Between News Headline an...   \n7   2019-03-11      Part of Speech Tagging for Code Switched Data   \n8   2019-03-11  Incorporating Label Dependencies in Multilabel...   \n9   2018-06-13  A Retrospective Analysis of the Fake News Chal...   \n\n                                                text  \\\n0  [a few topical tweets are enough for effective...   \n1  [stance detection benchmark how robust is your...   \n2  [stancexa multilingual multitarget dataset for...   \n3  [received january   accepted january   date of...   \n4  [inte rna tiona l j ournal of information te c...   \n5  [your stance is exp ose d analysing possible f...   \n6  [detecting incongruity between news headline a...   \n7  [part of speech tagging for code switched data...   \n8  [incorporating label dependencies in multilabe...   \n9  [a retrospective analysis of the fake news cha...   \n\n                                          clean_text  word_count  \\\n0  topical tweets enough effective stance detecti...        3205   \n1  stance detection benchmark robust stance detec...        3663   \n2  stancexa multilingual multitarget dataset stan...        3278   \n3  received january accepted january date publica...        3896   \n4  inte rna tiona l j ournal information te chnol...        4831   \n5  stance exp ose analysing possible factors fors...        7875   \n6  detecting incongruity news headline body textv...        4193   \n7  part speech tagging code switched data fahad a...        3089   \n8  incorporating label dependencies multilabel st...        1701   \n9  retrospective analysis fake news challengestan...        4911   \n\n                                             keyword  \n0  [expanded, darwish, retweeted, unsupervised cl...  \n1  [resilience, spelling, biases, attacks, std da...  \n2  [answers, french, switzerland, swiss, crosstar...  \n3  [focal, mechanism, focal loss, conf, sentimental]  \n4    [summarization, phological, classif, summaries]  \n5   [body text, incongruent, incongruence, ip, ahde]  \n6  [stancedete, predetection, tweets, performance...  \n7  [monolingual taggers, madamira, language pairs...  \n8        [bbc, mtlxld, mftc, multilabel, multiclass]  \n9  [headline document, headline, gaza, bound, ath...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>publish_date</th>\n      <th>title</th>\n      <th>text</th>\n      <th>clean_text</th>\n      <th>word_count</th>\n      <th>keyword</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2020-07-04</td>\n      <td>A Few Topical Tweets are Enough for Effective ...</td>\n      <td>[a few topical tweets are enough for effective...</td>\n      <td>topical tweets enough effective stance detecti...</td>\n      <td>3205</td>\n      <td>[expanded, darwish, retweeted, unsupervised cl...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2020-06-01</td>\n      <td>Stance Detection Benchmark: How Robust Is Your...</td>\n      <td>[stance detection benchmark how robust is your...</td>\n      <td>stance detection benchmark robust stance detec...</td>\n      <td>3663</td>\n      <td>[resilience, spelling, biases, attacks, std da...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2020-03-18</td>\n      <td>A Multilingual Multi-Target Dataset for Stance...</td>\n      <td>[stancexa multilingual multitarget dataset for...</td>\n      <td>stancexa multilingual multitarget dataset stan...</td>\n      <td>3278</td>\n      <td>[answers, french, switzerland, swiss, crosstar...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2020-01-27</td>\n      <td>Sentiment Classification Based on Part-of-Spee...</td>\n      <td>[received january   accepted january   date of...</td>\n      <td>received january accepted january date publica...</td>\n      <td>3896</td>\n      <td>[focal, mechanism, focal loss, conf, sentimental]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2020-01-03</td>\n      <td>Generating Summaries Through Unigram and Bigra...</td>\n      <td>[inte rna tiona l j ournal of information te c...</td>\n      <td>inte rna tiona l j ournal information te chnol...</td>\n      <td>4831</td>\n      <td>[summarization, phological, classif, summaries]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2019-08-08</td>\n      <td>Your Stance is Exposed! Analysing Possible Fac...</td>\n      <td>[your stance is exp ose d analysing possible f...</td>\n      <td>stance exp ose analysing possible factors fors...</td>\n      <td>7875</td>\n      <td>[body text, incongruent, incongruence, ip, ahde]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2019-08-02</td>\n      <td>Detecting Incongruity Between News Headline an...</td>\n      <td>[detecting incongruity between news headline a...</td>\n      <td>detecting incongruity news headline body textv...</td>\n      <td>4193</td>\n      <td>[stancedete, predetection, tweets, performance...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2019-03-11</td>\n      <td>Part of Speech Tagging for Code Switched Data</td>\n      <td>[part of speech tagging for code switched data...</td>\n      <td>part speech tagging code switched data fahad a...</td>\n      <td>3089</td>\n      <td>[monolingual taggers, madamira, language pairs...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2019-03-11</td>\n      <td>Incorporating Label Dependencies in Multilabel...</td>\n      <td>[incorporating label dependencies in multilabe...</td>\n      <td>incorporating label dependencies multilabel st...</td>\n      <td>1701</td>\n      <td>[bbc, mtlxld, mftc, multilabel, multiclass]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2018-06-13</td>\n      <td>A Retrospective Analysis of the Fake News Chal...</td>\n      <td>[a retrospective analysis of the fake news cha...</td>\n      <td>retrospective analysis fake news challengestan...</td>\n      <td>4911</td>\n      <td>[headline document, headline, gaza, bound, ath...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "# Mergering list to dataframe\n",
    "df['keyword'] = keyword\n",
    "df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above short analysis show the word count distribution over time and extracted word freqency and keyword over the year to know the trend.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text summarization\n",
    "\n",
    "There is different type of text summarization methods and libraries used for summarization such as NLTK, Spacy and deep learing. For this project I am using Spacy to demonstrate the Text summarization.\n",
    "\n",
    "### 1. Convert Paragraphs to Sentences\n",
    "We first need to convert the whole paragraph into sentences. The most common way of converting paragraphs to sentences is to split the paragraph whenever a period is encountered.\n",
    "\n",
    "### 2. Text Preprocessing\n",
    "After converting paragraph to sentences, we need to remove all the special characters, stop words and numbers from all the sentences.\n",
    "\n",
    "### 3. Tokenizing the Sentences\n",
    "We need to tokenize all the sentences to get all the words that exist in the sentences\n",
    "\n",
    "### 4. Find Weighted Frequency of Occurrence\n",
    "Next we need to find the weighted frequency of occurrences of all the words. We can find the weighted frequency of each word by dividing its frequency by the frequency of the most occurring word.\n",
    "\n",
    "### 5. Replace Words by Weighted Frequency in Original Sentences\n",
    "The final step is to plug the weighted frequency in place of the corresponding words in original sentences and finding their sum. It is important to mention that weighted frequency for the words removed during preprocessing (stop words, punctuation, digits etc.) will be zero and therefore is not required to be added\n",
    "\n",
    "### 6. Sort Sentences in Descending Order of Sum\n",
    "The final step is to sort the sentences in inverse order of their sum. The sentences with highest frequencies summarize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import heapq\n",
    "import warnings\n",
    "import logging\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger(\"lda\").setLevel(logging.WARNING)\n",
    "stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean text before feeding it to spaCy\n",
    "punctuations = '!\"#$%&\\'()*+,-/:;<=>?@[\\\\]^_`{|}~'\n",
    "# Define function to cleanup text by removing personal pronouns, stopwords, and puncuation\n",
    "def cleanup_text(docs, logging=False):\n",
    "    texts = []\n",
    "    doc = nlp(docs, disable=['parser', 'ner'])\n",
    "    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "    tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "    tokens = ' '.join(tokens)\n",
    "    texts.append(tokens)\n",
    "    return pd.Series(texts)\n",
    "\n",
    "\n",
    "df['clean_text_sp'] = df['clean_text'].apply(lambda x: cleanup_text(x, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text summarization\n",
    "\n",
    "def generate_summary(text_without_removing_dot, cleaned_text):\n",
    "    sample_text = text_without_removing_dot\n",
    "    doc = nlp(sample_text)\n",
    "    sentence_list=[]\n",
    "    for idx, sentence in enumerate(doc.sents): # we are using spacy for sentence tokenization\n",
    "        sentence_list.append(re.sub(r'[^\\w\\s]','',str(sentence)))\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    word_frequencies = {}  \n",
    "    for word in nltk.word_tokenize(cleaned_text):  \n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "\n",
    "\n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():  \n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "\n",
    "\n",
    "    sentence_scores = {}  \n",
    "    for sent in sentence_list:  \n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "\n",
    "\n",
    "    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    print(\"Original Text::::::::::::\\n\")\n",
    "    print(text_without_removing_dot)\n",
    "    print('\\n\\nSummarized text::::::::\\n')\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Original Text::::::::::::\n\ntopical tweets enough effective stance detectionyounes samih kareem darwishqatar computing research institutehamad bin khalifa university doha qatar ysamihkdarwish detection entails ascertaining position target entity topic claim employs unsupervised classiﬁcation performing stance detection vocal twitter users rahave many tweets target yield high however methods perform poorly fail comapletely less vocal users may authored tweets target paper tackle stance users using two approaches ﬁrst ap proach improve userlevel stance detection representbiing tweets using contextualized embeddings captureslatent meanings words context show apfigure sample us midterm election related tweetsproach outperforms two strong baselines achieves macro fmeasure eight controversialcthat either express clear stance btopics second approach expand tweets given user using twitter timeline tweets unsupervised classiﬁcation user enoptimal results users rarely express opinionvtails clustering user users training setand may one two topically approach achieves accuracy macro flated tweets though single tweet might explicitly may lack sufﬁcient context determine user figure show two tweets pertain us midterm elections ﬁrst expresses lucidprorepublican stance second could austance detection entails identifying position supporter either republican democratictowards topic entity claim mohammad et paper aim effectively identify effective stance detection particularly twitter users towards speciﬁc targets entities topicsof social media instrumental gauging public opinwhere users mentioned targets fewion identifying intersecting diverging groups unvtweets less two tweets averagederstanding issues interest different user communiixto employ two approaches ﬁrst apties magdy et al much recent works explored varying stance detection methods including super proach classify user based tweets repravised semisupervised unsupervised user classiﬁcation resented using contextualized embeddings capturedarwish et al magdy et al pennacchiotti latent meanings words context speciﬁcally usepopescu wong et al much work bert embeddings represent tweets ﬁne tune thehas focused stance detection twitter users dif embeddings every topic compare approach toferent approaches advantages disadvantages two strong baselines namely using support vector machineexample supervised methods simple implement svm classiﬁcation fasttext deep learnthey require manually annotated training data accu ing based classiﬁer second approach expand theracy varies widely based classiﬁcation features clas tweets given user using twitter timeline tweetsand perform unsupervised classiﬁcation usersiﬁcation technique number training test examples magdy et al though semisupervised clustering himher users training set usunsupervised methods typically use user interactions ing expansion allows us make use echo chambersten may yield perfect classiﬁcation effective clas form twitter users similar views tendsifying highly vocal users many topical tweets dar retweet similar accounts beyond topic hand testwish et al methods produce sub approaches used dataset containing tweets po larizing uscentric topics also examine effect ex methods one major downsides classiﬁcationpansion using svm fasttext contextualized em methods need seed list manually labeled usersbeddings testing randomly selected users time consuming requires topic expertise sueach topic less topical tweets man pervised learning sensitive classiﬁcation featuresually labeled stance construct training set size training sets number available tweetswe used unsupervised stance detection automatically la users test set classiﬁcation algorithmbel active users per topic every topic borgeholthoefer et al common classiﬁcawe used balanced set users per stance training tion features include lexical syntactic semantics feaset darwish et al since approaches rely dif ture network features retweeted accounts userferent features utilize different classiﬁcation techniques mentions content features words hashtags andwe indicate approach works best different con user proﬁle information name location aldayelditions magdy magdy et al magdy darwishand weber pennacchiotti popescu somethe contributions paper follows commonly used classiﬁcation algorithms include svms andwe ﬁnetune contextualized embeddings generate ladeep learning classiﬁcation zarrella marsh representations tweets effectively classify thepopat et al present neural network model stancestance users based one two tweets weclassiﬁcation augmenting bert representations aachieve fmeasure arenovel consistency constraint determine stance resigniﬁcantly h igher scores achieved bothspect claim perspective extend workbaselinesin two ways namely drop need claim show using additional timeline tweets theand perspective couple bert supervised classiﬁcausers wish classify using unsup ertion unsupervised classiﬁcation effectively tag vocalvised classiﬁcation cluster test user withlabeland nonvocal users semisupervised methods asusers training leads accuracy andpropagation barbera borgeholthoefer et al extend prior workweber garimella batayneh often rely twoon unsupervised stance detection effectively classifyusers retweeting identical accounts tweets propagate aboth users vocal topic well withlabel one user another though typically achievesperhaps one two topical tweetshigh precision often darwish et al generally successful tagging vocal users strongwe conduct error analysis best setups determineopinions recently darwish et al introducedthe sources errors would help guiding thea highly effective unsupervised method predicting thechoice classiﬁcation methodstance proliﬁc twitter users towards controversial topics plan release tweet ids test set along withby projecting users onto lowdimensional space thenthe associate gold labels plan release theclustering allows clear separation vocalcode performs classiﬁcation based contextualizedusers respect stance darwish et al thisembeddingsmethod confers two main advantages previous methods namely require initial manual labelrelated working classiﬁcation accuracy nearly perfect howeverit successful labeling vocal users fails usersover last years much research focused onwith topical tweets exten prior work unstance detection goal stance detection ascersupervised stance detection effectively classify protain positions users towards target aliﬁc nonproliﬁc users holistic way aggregatingtopic person claim thomas pang lee moboth supervised unsupervised methods ex hammad et al barbera barbera riverotend upon prior deeplearning based supervised borgeholthoefer et al cohen ruthstion use contextual embeddings capture syntactic colleoni rozza arvidsson conover etsemantic features words contextal fowler et al himelboim mccreery andsmith magdy et al magdy darwish wedata setsber makazhanov raﬁei waqar webergarimella batayneh stance may easily detected humans machinelearning models often fall short particularly users dataset includes tweets eight polarizing topics thattalk target sparingly several studies focused uscentric graciously provided us stefanov et al stefanov darwish nakov table modeling stance introducing different features ranging linguistic structural features mohammad et lists topics including tweets collectedal way network interactions proﬁle number tweets per topic topics include bothinformation borgeholthoefer et al magdy et al longstanding issues gun control transient magdy darwish weber weber garimella sues nomination judge kavanaugh usand batayneh much work stance detection supreme court also nonpolitical issue namelyvolved using supervised semisupervised classiﬁcation vaccination tweets also ﬁltered based user stated locations limit data us users ﬁlteringwas done using gazetteer includes either us itsvariants state names abbreviationstopic date range tweetsclimate change feb mar finetuning bert stance classiﬁcationgun control feb mar omar remarks israel lobby mar labeled users user timelines scrapedmidterm elections feb mar change nomination supreme sept oct control omar mar controversial topics used studyracism police settable per topic labeled users test set along thegiven tweets every topic performed per topic unnumber users able scrape timelinessupervised stance detection darwish et al approach identiﬁes active users per topic comnputes similarity based common featureusers without timeline tweets shall report sepasuch hashtags use accounts theyrately experiments speciﬁcally put usersretweet next users projected onto lower dimenfor able collect timeline tweets intosional space manner similar users broughtset put remaining users set bcloser together dissimilar users pushed apartthen projected users clustered using best reclassiﬁcation modelsported setup darwish et al used activesupervised users least tweets computed similarity based accounts retweeted probaselinesas baselines used two different classiﬁcajected users using umap mcinnes healy andtion methods namely support vector machines svm clasclustered using mean shift clustering algorithmsiﬁer deep learning based text classiﬁer svmstefanov et al stefanov darwish nakov esticlassiﬁer used svm implementation linlig htmated accuracy approach kernel default parameters joachims weto next took random users twoemployed two feature types namely accounts userslargest clusters construct balanced training set weretweeted words tweets including retweeted acmanually inspected users cluster give ancounts hashtags user mentions replies prior workoverall label cluster ex pro anti gun controlhas shown using retweeted accounts features yieldsfurther crawled timeline tweets users ourbetter results compared using content tweet dartraining setwish et al using words tweets tokenized tweets using nltk removed urls emotitest setcons retained hashtags user mentions speciﬁfor topic randomly selected users havecally delineated retweeted accounts adding rt beforeless tweets average number tweets per userthem chose distinguish retweeted accountsranged\n\n\nSummarized text::::::::\n\nclassiﬁcation algorithmbel active users per topic every topic borgeholthoefer et al common classiﬁcawe used balanced set users per stance training tion features topically approach achieves accuracy macro flated tweets though single tweet might explicitly may lack sufﬁcient context determine user figure show two tweets pertain us midterm elections labeled users test set along thegiven tweets every topic performed per topic unnumber users able scrape timelinessupervised stance detection darwish et al much recent works explored varying stance detection methods including super proach classify user based tweets repravised effectively tag vocalvised classiﬁcation cluster test user withlabeland nonvocal users semisupervised onwith topical tweets exten prior work unstance detection goal stance detection ascersupervised stance detection effectively classify protain positions users towards target aliﬁc nonproliﬁc users holistic way aggregatingtopic person claim users towards controversial topics plan release tweet ids test set along withby projecting users onto lowdimensional space\n"
    }
   ],
   "source": [
    "generate_summary(df['clean_text'][0][:10000], df['clean_text_sp'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Original Text::::::::::::\n\nstance detection benchmark robust stance detectionbenjamin schiller johannes daxenberger iryna gurevychubiquitous knowledge processing lab ukptuda department computer science technische universitat darmstadt darmstadtdeabstracttopicclimate change real concerntweetgone days would getstance detection std aims detect autemperatures min max cape townsemstthor stance towards certain topic claimstancefavorand become key component like fake news detection claim task long tradition domain argument search however whilelitical ideological online debates easily detected humans machine et al walker et al somasundarannlearning models clearly falling short thisaand wiebe thomas et al recenttask given major differences datasetjsizes framing std eg number ofyears brought focus attention inputs introduce std benchby uprising debates around fake news mark learns ten std datasets ofstd important preprocessing step pomerleaulmultidataset learningvarious domains aand rao derczynski et al ferreiracmdl setting well related tasksand vlachos well downvia transfer learning within benchmarksstream tasks like argument search stab et al able present new stateofthecand claim validation popat et al suchart results ﬁve datasets yet mod high performance std crucial step sucels still perform well human even simple adversarial attacks severelyvcessfully leveraging machine learning ml performance mdl models deeperargumentative information retrieval fake phenomen existence biases inherited humans quite capable ofdatasets design analysis correct stances ml models oftenthe need focus robustness short task see table arestrategies multitask learning domains std appliedthe benchmark dataset code made task vary considerably instance ﬁrst input short topic introductionior sometimes given second inxput another claim evidence even fullstance detection std represents wellraargument second input differ inestablished task natural language processinglength sentence short paragraph andand often described two inputs documents number classes alsoa topic discussion comment madefor againstvary problems eg author given two inputs aim toﬁnd whether author favor againstthe topic instance task stateoftheart agreementmohammad et al second input aarc habernal et al pomerleau rao tweet goal detect whether theperspectrum chen et al made positive negative commenttowards given controversial topictable interannotator agreement iaa vs stateoftheart results f macro f micro iaa hanselowski et al stance comand ﬁnegrained problems eg achieve new stateoftheart results ﬁve ofment support query deny moreover number ten datasets indepth analysis adverof samples varies drasticially datasets sarial attacks show tl mdl stdour setup generally improves performance ml modelsdifferences problematic crossdomain per also drastically reduces robustness comformance also seen advantage pared sdl models foster analysisit concludes abundance datasets dif task publish full benchmark systemferent domains integrated transfer including model training evaluation well asor multitask learning approaches yet given means add evaluate adversarial human performance task hard sets low resource experiments datasetsgrasp ml models fall short std ﬁnetuned models machine translationare almost par related tasks like sentiment models automatically downloaded natural language inference nli processed consistent future usagewithin work provide foundations related workanswering question empirically assesswhether abundance differently framed stdstance detectionis wellestablished task natdatasets multiple domains leveraged byural language processing initial work focused onlooking holistic way ie training andparliamentary debates thomas et al andevaluating collectively multitask fashiondebating portals somasundaran wiebe one task multiplewhereas latest work shifted domain ofdatasets henceforth deﬁne multidatasetsocial media several shared tasks beenlearning mdl indeed model proﬁtsintroduced gorrell et al derczynski et alsigniﬁcantly datasets task mohammad et al shiftmdl percentage points pp averagein domains deﬁnition task also shiftedas well related tasks via transfer learningquerymore classes added eg gorrell et altl pomerleau rao gain signiﬁcant performancethe number inputs changed eg multipleimprovements std using tl mdl thetopics sample sobhani et al orexpected robustness approaches missingthe deﬁnition inputs eg parliarewe show using modiﬁed version thementary speeches debate portal posts tweetssiliencescore thorne et al revealsgorrell et al news articles pomerleau andthat tl mdl models even less robust thanrao argument components stab et alsingledataset learning sdl models barhaim et al past years thegate phenomenon low resource experiproblem std become cornerstone manyments observe less training data leads todownstream tasks like fake news detection pomeran improved robustness mdl models narleau rao claim validation popat et alrowing gap sdl models argument search stab et al yetassume lower robustness stems datasetrecent work mainly focuses individual datasetsbiases introduced vast amount availableand domains contrast concentrate atraining data mdl models leading overhigher level abstraction aggregating datasetsﬁtting consequently adversarial attacks targetof different domains deﬁnitions analyzesuch biases severe impact modelsthem holistic way leveragethat biased training data overﬁttedthe idea tl multitask learning formon biasesof mdl shown increases inthe contributions paper followsperformance robustness ruder best knowledge ﬁrst toet al also signiﬁcant support lowcombine learning related tasks via tl andresource scenarios schulz et al latestmdl designed capture facets std tasksframeworks multitask learning include liu et al scored new statesentimentanalysishtmloftheart glue benchmark wang et contrast work use thenaturallanguageinferencehtml dataset groups domain exampletopic comment stanceibmcsencyclopedia atheism way atheism superior basis ethics charlie hebdo suspected charliehebdo gunmen killed yayyyboom supportsocial feminist movement believe every women rights semst russia partner america russia iran come together start agreenewssnopes farmers feed cattle candy alternative would put landﬁll somewhere agreescd obama think obama great president forperspectrum debating school day extended much easier parents forums existence god bible tells jesus existed son god proarc salt place table iodine salt necessary prevent goiter agreeargminweb search school uniforms believe freedom choice contable datasets grouped domain examples topics parentheses signal implicit datasetsframework mdl ie combining datasetsof task analyze whether std datasetswe choose ten std datasets ﬁve different docan beneﬁt transferring knowlmains represent rich environment differentedge domains furthermore probefacets std datasets within one domain maythe robustness learned models analyzestill vary number classes samplewhether performance increases gained tlsizes datasets shown example andand mdl accordance increased robusttheir domain table addition table disness stdplays split sizes class distributions ofadversarial attacksdescribe test sets aimed diseach dataset code preprocess split thecover possible weak points ml models available online following allmuch recent work adversarial attacks aims todatasets introducedbreak nli systems especially adapted toarcwe take version argument reasoningthis problem glockner et al minervinicorpus habernal et al modiﬁedand riedel stress tests beenfor std hanselowski et al sampleapplied wide range tasks questionconsists claim crafted crowdworker aanswering wang bansal naturaluser post debating forummachine translation belinkov bisk ukp sentential argument miningand fact checking thorne et al unforcorpus stab et al originally contains topictunately preserving semantics sentenceargument argusentence pairs labelled automatically generating adversarialment argument remove allattacks difﬁ cult works havenonarguments simplify original split wedeﬁned small stress tests manually isabelle et altrain data ﬁve topics develop mahler et al time andof one topic test data two topicsmoney consuming work deﬁned fake news challenge dataset pomerleautics controllable outcome modify existingand rao contains headlinearticle pairs fromdatasets preserve semantics datanews websites take original data withoutnaik et al contrast previous workmodifying itwe use analyze attacks internet argument corpus walkertask std probe robustness sdl andet al contains topicpost pairs politimdl modelscal debates internet forums generate stance detection benchmark setupsplit without intersection topics trainand experimentsdevelopment test ibm debater claim stance datasetrwe describe dataset models use forbarhaim et al contains topicclaim pairsthe benchmark experimental setting thethe topics gathered debating databaseresults experiments experiments wethe claims manually collected adapt framework provided liu et alarticles take predeﬁned train test split additional train set dnn perspectrumthe perspectrum dataset chenmtd nn mtd nnsdl mdlet al contains pairs claims relatedsingle std setall std setsperspectives gathered debatingmtd nnwebsites take data deﬁned forthe std task work keep exact splitbertbertscdsdlglue stance classiﬁcation dataset hasan andmdlbenchmarksingle ng contains posts four topics fromstd et std etsan online debate forum posts selfbertlabelled post author topics notpart actual dataset inferredfigure models relation arrows symbolizefrom explicit implicit mentions within posttraining labels state used training datawe generate new data split using data oftwo topics training data one topic forture shared datasetspeciﬁc layers updevelopment data leftover topic fordated dataset individually training timetestingall datasets batched fed task datasettecture random order initial weights formohammad et al contains topictweetsdl mdl use either pretrained bertpairs topics controversial subjects likelarge uncased weights devlin et al orpoliticians \n\n\nSummarized text::::::::\n\nsigniﬁcant performancethe number inputs changed eg multipleimprovements std using tl mdl thetopics sample sobhani et al orexpected robustness approaches missingthe deﬁnition inputs eg parliarewe show using modiﬁed version et alsigniﬁcantly datasets task mohammad et al shiftmdl percentage points pp averagein domains deﬁnition task et al time andof one topic test data two topicsmoney consuming work deﬁned fake news challenge dataset pomerleautics fake news detection pomeran improved robustness mdl models narleau rao claim validation popat et alrowing gap sdl models argument search stab et et al news articles pomerleau andthat tl mdl models wide range tasks questionconsists claim crafted crowdworker aanswering wang bansal naturaluser post debating forummachine translation belinkov bisk ukp sentential argument miningand fact checking thorne et al unforcorpus stab\n"
    }
   ],
   "source": [
    "generate_summary(df['clean_text'][1][:10000], df['clean_text_sp'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Original Text::::::::::::\n\nstancexa multilingual multitarget dataset stance detectionjannis vamvas rico computational linguistics university informatics university paper propose much larger datasetthat combines multilinguality multitude ofwe extract largescale stance detection datatopics targets stance comprises thanxset comments written candidates questions concerning swiss politics moreelections switzerland dataset answers given last decade german french italian text running political ofﬁce switzerlandlowing crosslingual evaluation stance questions available four languages enrdetection contains comments onatargetsmore political issues unglish swiss standard german french italmlike stance detection models speciﬁcian language comment depends target issues use dataset train region model issues make learnwe extracted data voting ad ing across targets possible prepend application platforminstance natural question represents thelcandidates respond questions mainly categortarget eg support x baseline recical form yes rather yes rather theysults multilingual b show zeroe r tsshot crosslingual crosstarget transfer ofcan also submit freetext comment order tocstance detection moderately successful withjustify explain differentiate categorical approach swer example given figure transform dataset stance task interpreting question recent years many datasets crelanguage representation target task automated stance detectionmentary input natural language understanding systhe dataset split multilingual political science opinion research andset multiple test sets evaluate application areas typically benchcrosslingual crosstarget transfer mohammad et al composedvide baseline ﬁnetune multilingual r tof short pieces text commenting politiciansmodel devlin et al stance showvxior public issues manually annotated withthat baseline accuracy comparable prextarget entitytheir stance towards eg climatevious stance detection benchmarks leavingrachange trump however limited inample room improvement addition mulscope multiple levels küçük b generalize degree bothe r tfirst questionable well curcrosslingually crosstarget settingrent stance detection methods perform crosswe made dataset code relingual setting multilingual datasets baseline model publicly availableable today relatively small speciﬁc related worksingle target taulé et al furthermore speciﬁc models tend developed formultilingual stance detectionin context ofeach single target pair targets sobhani et althe ibereval shared tasks two related concerns raised crosstargetperformance often considerably lower performance küçük question available languagessoll der bundesrat ein frei la suisse devraitelle conclure unshould switzerland strive freehandelsabkommen mit den usa accord de libreéchange avec lestrade agreement usaanstreben etatsuniscomment german comment frenchlabel favor label againstmit unserem zweitwichtigsten handels les accords de libreéchange menacent lapartner sollten wir ein freihandels qualité des produits suissesabkommen habenwith second important trading free trade agreements jeopardize thepartner free trade quality swiss productsagreementfigure example question two answers stance dataset answers submitted electoralxcandidates voting advice website author comment german declared favor issueand added comment explanation comment french written another candidate toexplain negative stance taken towards issue stance contains hundreds answers toxthis question picked two comments examples due brevitymultitargetdatasets created taulé et al sobhani et al introduced collection annotated spanish stance dataset provides two targets per inand catalan tweets crucially tweets stance example model designed thislanguages focus issue catalan inde framework supposed simultaneously classifypendence given fact ﬁrst truly tweet regard clinton regard tomultilingual stance detection datasets known us trump theory framework allows formore two targets still restricted ﬁwith regard languages covered bynite clearly deﬁned set targets focusesstance monolingual datasets seem bexon modeling dependencies multiple targetsavailable french collection tweets onwithin text sample approachfrench presidential candidates annotatedfocuses learning stance detection manywith stance lai similarly two datasets ofsamples many different targetsitalian tweets occasion constitutional referendum created lai et alrepresentation learning stance lai german knownin targetspeciﬁc setting ghosh et al detection dataset aspectbased senperform systematic evaluation stance detectiment dataset created germevaltion approaches also evaluate b shared task wojatzki et al r tet al ﬁnd consistently outperforms previous approachesmultitarget stance detectionthe task detecting stance tweets moham however experimented singlemad et al offers data concerning multi segment encoding input preventing crossple targets atheism climate change feminism target transfer model augenstein et alhillary clinton abortion supervised propose conditional encoding approachtargetto encode target tweet sesubtask participants tended develop aspeciﬁcmodel targets sub quences use bidirectional lstm conditask b crosstarget transfer target donald tion encoding tweets encoding oftrump tested annotated train target apply nonlinear projection oning data provided required conditionally encoded tweet allows themto train model generalize previouslydevelopment universal models performance generally much lower unseen targets smartvotethe questions asked beentopic questions answersedited team political scientists aredigitisation cover broad range political iseconomy relevant time election deeducation documentation design offinances editing process questions providedforeign policy thurman gasser merged two labels oninfrastructure environment pole single label yes rather yes security combined favor rather society improves consistency ofwelfare training topics data comparability previous stancedetection datasetshealthcare preprocess text thepolitical system heldout topics identiﬁcationas api notprovide language comments employedtable number questions answers per topica language identiﬁer automatically annotatelangdetectthis information used stance datasetxbrary shuyo responder classiﬁed comments jointly assuming task deﬁnitionsponders switch code answeringthe input provided stance twofold axof questionnairea natural language question concerning politiwe applied identiﬁer twostep approachcal issue b natural language commentary onin ﬁrst run allowed identiﬁer outa speciﬁc stance towards questionput languages supports thethe label predicted either favor orbox plus romansh fourth ofﬁcial language corresponds standard found romansh comlished mohammad et al howeverments detected unexpected outstance differs dataset lacks axputs misclassiﬁcations german french neither class comments refer either faor italian comments concluded thatvor position task posed bylittle swiss german comments thestance thus binary classiﬁcation taskxdataset would haveas evaluation metric report macromanifested form misclassiﬁaverage favor eg dutchscore similar mohammad et alin second run drawing use metric mainly strengthensions restricted identiﬁer output encomparability previous benchmarksglish french german data collectionfilteringwe preﬁltered questions answers improve quality dataset toprovenancewe downloaded questions ankeep domain data surveyable set asmartvoteswers via api downloadedfocus nationallevel questions therefore alldata cover communal cantonal nationalquestions corresponding answers pertaining toelections elections includedall candidates election participate inin context communal cantonal elecsmartvoteare asked set questions buttions candidates answered local quesdepending locale see translated versions questions answer rumantsch grischun variety language proﬁle created using resources thequestion either yes rather yes rather zurich parallel corpus collection graën et al supplement answer aquotidianaand corpus comment charactersprosvizrarumantschacorpora intratarget crossquestioncrosstopicnew answers new questionsknown questions within known topicstrain test evalid test rvalid test test tupper left cornertable number answer instances training validation test sets represents atopmultilingually supervised task training validation test data exactly domain thetobottom axisgives rise crosslingual transfer task model trained german french evaluatedlefttoright axison italian answers questions represents continuous shift domain themiddle column model tested previously unseen questions belong topics seen duringright columntraining model encounters unseen answers unseen questions within unseen topicthe two test sets parentheses small signiﬁcant evaluationtions subset national questions given sensitive nature data inthose elections considered answers crease anonymity data hashingquestions also asked national respondents ids personal attributes ofelection furthermore used aug respondents party afﬁliationment training set validation test included dataset provide datasets restricted answers national elec statement bender friedman aptions pendix bwe discarded less comments data splitsiﬁed english furthermore instances metany following conditions ﬁltered fromwe held topics healthcare politicalthe datasetsystem training data created sepacrosstopicrate test set contains questions question closed question notand answers related topicsaddress clearly deﬁned political issuecrossquestionfurthermore order test generalization performance within previously seenno comment submitted candidatetopics manually selected heldout quesor comment shorter characterstions distributed remaining comment starts similar topics selected heldout questions mancator comment selfcontainedually wanted make sure arestatementtruly unseen paraphrases ques tions found training setcomment contains urlwe designated italian testonly languagein total ﬁfth original comments weresince relatively comments writtenﬁltered outin italian remaining german frenchdata randomly \n\n\nSummarized text::::::::\n\nansweringthe input provided stance twofold axof questionnairea natural language question concerning politiwe applied identiﬁer twostep approachcal issue b natural language commentary onin et al detection dataset aspectbased senperform systematic evaluation stance detectiment dataset created germevaltion approaches also evaluate b shared task wojatzki et al r tet al ﬁnd consistently outperforms previous approachesmultitarget stance detectionthe task detecting stance tweets moham however experimented singlemad political ofﬁce switzerlandlowing crosslingual evaluation stance questions available four languages enrdetection contains comments onatargetsmore political issues unglish swiss standard german french italmlike stance detection models well curcrosslingually crosstarget settingrent stance detection methods perform crosswe made dataset code relingual setting multilingual datasets baseline model publicly availableable today moderately successful withjustify explain differentiate categorical approach swer example given figure transform dataset stance task interpreting question recent years represents atopmultilingually supervised task training validation test data exactly domain thetobottom axisgives rise crosslingual transfer task model trained german french evaluatedlefttoright\n"
    }
   ],
   "source": [
    "generate_summary(df['clean_text'][2][:10000], df['clean_text_sp'][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** We successfully develop a model can summaries the text by running above fuction will give as summaries text.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Main Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "[('andrea hanselowski', 'PERSON'),\n ('benjamin schiller', 'PERSON'),\n ('debanjan', 'PERSON'),\n ('christian meyer iryna', 'ORG'),\n ('science department technische universität darmstadtsmart', 'ORG'),\n ('data analytics university', 'ORG'),\n ('bonn', 'GPE'),\n ('three', 'CARDINAL'),\n ('ﬁnd', 'ORG'),\n ('ai technology automatically detect fake news challenge', 'ORG'),\n ('nlp community', 'ORG'),\n ('four', 'CARDINAL'),\n ('essay stab gurevych', 'PERSON'),\n ('fourth', 'ORDINAL'),\n ('three', 'CARDINAL'),\n ('palestinians', 'NORP'),\n ('israel', 'GPE'),\n ('damsagreegaza city', 'GPE'),\n ('hundred', 'CARDINAL'),\n ('palestinians', 'NORP'),\n ('sunday', 'DATE'),\n ('morning', 'TIME'),\n ('israeli', 'NORP'),\n ('ﬂoode gaza valley', 'GPE'),\n ('recenta g', 'PERSON'),\n ('gaza ministry', 'ORG'),\n ('gaza', 'GPE'),\n ('three meter', 'QUANTITY'),\n ('hundred', 'CARDINAL'),\n ('israel', 'GPE'),\n ('gaza strip', 'GPE'),\n ('israel', 'GPE'),\n ('gaza strip', 'GPE'),\n ('israel', 'GPE'),\n ('palestinian', 'NORP'),\n ('thegaza valley wadi', 'LOC'),\n ('gaza', 'GPE'),\n ('almost three meter', 'QUANTITY'),\n ('somasundaran wiebe', 'PERSON'),\n ('sridhar et al neural', 'PERSON'),\n ('zarrella marsh', 'PERSON'),\n ('al conditional', 'FAC')]\n[('institutehamad bin khalifa university', 'PERSON'),\n ('qatar', 'GPE'),\n ('two', 'CARDINAL'),\n ('two', 'CARDINAL'),\n ('eight', 'CARDINAL'),\n ('second', 'ORDINAL'),\n ('two', 'CARDINAL'),\n ('two', 'CARDINAL'),\n ('second', 'ORDINAL'),\n ('republican', 'NORP'),\n ('two', 'CARDINAL'),\n ('two', 'CARDINAL'),\n ('et al magdy', 'PERSON'),\n ('usepopescu wong', 'PERSON'),\n ('two', 'CARDINAL'),\n ('second', 'ORDINAL'),\n ('us', 'GPE'),\n ('et al', 'PERSON'),\n ('popescu somethe contributions paper', 'ORG'),\n ('svms andwe ﬁnetune', 'ORG'),\n ('zarrella marsh', 'PERSON'),\n ('one two', 'CARDINAL'),\n ('two', 'CARDINAL'),\n ('theand', 'GPE'),\n ('bert', 'PERSON'),\n ('withlabeland nonvocal users', 'ORG'),\n ('one two', 'CARDINAL'),\n ('two', 'CARDINAL'),\n ('last years', 'DATE'),\n ('thomas pang lee moboth', 'PERSON'),\n ('al cohen', 'PERSON'),\n ('colleoni rozza arvidsson', 'PERSON'),\n ('makazhanov raﬁei', 'PERSON'),\n ('batayneh', 'NORP'),\n ('eight', 'CARDINAL'),\n ('stefanov et al stefanov darwish', 'ORG'),\n ('nakov', 'PERSON'),\n ('mohammad et', 'PERSON'),\n ('weber weber', 'DATE'),\n ('kavanaugh usand', 'PERSON'),\n ('ﬁlteringwas', 'ORG'),\n ('feb mar', 'PERSON'),\n ('bert stance classiﬁcationgun', 'PERSON'),\n ('feb mar omar', 'PERSON'),\n ('israel', 'GPE'),\n ('mar', 'PERSON'),\n ('feb mar', 'NORP'),\n ('omar mar controversial', 'PERSON'),\n ('thegiven', 'PERSON'),\n ('bcloser', 'PERSON'),\n ('et al', 'PERSON'),\n ('two', 'CARDINAL'),\n ('classiﬁer svmstefanov et al stefanov', 'PERSON'),\n ('nakov esticlassiﬁer', 'PERSON'),\n ('weto', 'ORG'),\n ('two', 'CARDINAL'),\n ('ﬁnd', 'ORG'),\n ('joulin et al sinceto', 'PERSON'),\n ('heshe retweeted', 'ORG'),\n ('umap', 'GPE'),\n ('vectimeline', 'ORG'),\n ('us', 'GPE'),\n ('svm svm', 'PERSON'),\n ('et al bert', 'PERSON'),\n ('algorithm notrepresentations', 'ORG'),\n ('devlin et al', 'PRODUCT'),\n ('theand umlfit', 'PERSON'),\n ('howard ruder', 'PERSON'),\n ('nlp', 'ORG'),\n ('four', 'CARDINAL'),\n ('timelinebert ﬁnetune stance', 'ORG'),\n ('bert', 'PERSON'),\n ('hundreds millions', 'CARDINAL'),\n ('earlier forsvm', 'DATE'),\n ('roberta liu et', 'PERSON'),\n ('xlm lample', 'PERSON'),\n ('bert perbasemultilingualwhen', 'PERSON'),\n ('svm bertr', 'PERSON'),\n ('notgun', 'NORP'),\n ('testnra', 'GPE'),\n ('million', 'CARDINAL'),\n ('ali', 'PERSON'),\n ('one', 'CARDINAL'),\n ('jeff ﬂakestances', 'PERSON'),\n ('unusablert somewe', 'PERSON'),\n ('poland', 'GPE'),\n ('examplesall', 'ORG'),\n ('bert fasttext', 'PERSON'),\n ('bert', 'PERSON'),\n ('republican', 'NORP'),\n ('bert', 'PERSON'),\n ('four', 'CARDINAL'),\n ('examinedpolitbayesian', 'NORP'),\n ('egyptian', 'NORP'),\n ('three', 'CARDINAL'),\n ('stron', 'NORP'),\n ('ieee', 'ORG'),\n ('third', 'ORDINAL'),\n ('republican', 'NORP'),\n ('national riﬂe association', 'ORG'),\n ('nra', 'ORG'),\n ('k magdy', 'PERSON'),\n ('islamophobic', 'NORP'),\n ('parisattacksthus', 'NORP'),\n ('aupetit j nakov', 'PERSON'),\n ('chang', 'PERSON'),\n ('two', 'CARDINAL'),\n ('north american', 'NORP'),\n ('two', 'CARDINAL'),\n ('minnesota association', 'ORG'),\n ('second', 'ORDINAL'),\n ('evenamerican', 'NORP'),\n ('howard', 'PERSON'),\n ('us', 'GPE'),\n ('sup international joint conference', 'ORG'),\n ('ms processing', 'ORG'),\n ('pdflan', 'GPE'),\n ('chen goodman', 'PERSON'),\n ('albert', 'PERSON'),\n ('theliu ott', 'PERSON'),\n ('joshi chen', 'PERSON'),\n ('lewis zettlemoyer', 'PERSON'),\n ('bert', 'PERSON'),\n ('australia association', 'ORG'),\n ('islamist', 'NORP'),\n ('egypt', 'GPE'),\n ('kiritchenko sobhani', 'PERSON'),\n ('kiritchenko sobhani', 'PERSON'),\n ('zhu xand', 'PERSON'),\n ('popescu democratsrepublicans', 'ORG'),\n ('neumann iyyer', 'PERSON'),\n ('clarkc lee k zettlemoyer', 'PERSON'),\n ('yates weikum g stance', 'PERSON'),\n ('onlya', 'GPE')]\n"
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "pprint([(X.text, X.label_) for X in doc.ents])\n",
    "\n",
    "# Extracting named entity from an article\n",
    "tagged_text = nlp(df['clean_text'][0])\n",
    "extracted_entities = [(i.text, i.label_) for i in tagged_text.ents]\n",
    "\n",
    "pprint(extracted_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "Counter({'PERSON': 55,\n         'GPE': 12,\n         'CARDINAL': 23,\n         'ORDINAL': 5,\n         'NORP': 14,\n         'ORG': 21,\n         'DATE': 3,\n         'PRODUCT': 1})"
     },
     "metadata": {},
     "execution_count": 114
    }
   ],
   "source": [
    "# Unique Lables\n",
    "labels = [i.label_ for i in tagged_text.ents]\n",
    "Counter(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Extracted for label: NORP\n- republican\n\n\n- batayneh\n\n\n- feb mar\n\n\n- notgun\n\n\n- republican\n\n\n- examinedpolitbayesian\n\n\n- egyptian\n\n\n- stron\n\n\n- republican\n\n\n- islamophobic\n\n\n- parisattacksthus\n\n\n- north american\n\n\n- evenamerican\n\n\n- islamist\n\n\nExtracted for label: PERSON\n- institutehamad bin khalifa university\n\n\n- et al magdy\n\n\n- usepopescu wong\n\n\n- et al\n\n\n- zarrella marsh\n\n\n- bert\n\n\n- thomas pang lee moboth\n\n\n- al cohen\n\n\n- colleoni rozza arvidsson\n\n\n- makazhanov raﬁei\n\n\n- nakov\n\n\n- mohammad et\n\n\n- kavanaugh usand\n\n\n- feb mar\n\n\n- bert stance classiﬁcationgun\n\n\n- feb mar omar\n\n\n- mar\n\n\n- omar mar controversial\n\n\n- thegiven\n\n\n- bcloser\n\n\n- et al\n\n\n- classiﬁer svmstefanov et al stefanov\n\n\n- nakov esticlassiﬁer\n\n\n- joulin et al sinceto\n\n\n- svm svm\n\n\n- et al bert\n\n\n- theand umlfit\n\n\n- howard ruder\n\n\n- bert\n\n\n- roberta liu et\n\n\n- xlm lample\n\n\n- bert perbasemultilingualwhen\n\n\n- svm bertr\n\n\n- ali\n\n\n- jeff ﬂakestances\n\n\n- unusablert somewe\n\n\n- bert fasttext\n\n\n- bert\n\n\n- bert\n\n\n- k magdy\n\n\n- aupetit j nakov\n\n\n- chang\n\n\n- howard\n\n\n- chen goodman\n\n\n- albert\n\n\n- theliu ott\n\n\n- joshi chen\n\n\n- lewis zettlemoyer\n\n\n- bert\n\n\n- kiritchenko sobhani\n\n\n- kiritchenko sobhani\n\n\n- zhu xand\n\n\n- neumann iyyer\n\n\n- clarkc lee k zettlemoyer\n\n\n- yates weikum g stance\n\n\nExtracted for label: ORG\n- popescu somethe contributions paper\n\n\n- svms andwe ﬁnetune\n\n\n- withlabeland nonvocal users\n\n\n- stefanov et al stefanov darwish\n\n\n- ﬁlteringwas\n\n\n- weto\n\n\n- ﬁnd\n\n\n- heshe retweeted\n\n\n- vectimeline\n\n\n- algorithm notrepresentations\n\n\n- nlp\n\n\n- timelinebert ﬁnetune stance\n\n\n- examplesall\n\n\n- ieee\n\n\n- national riﬂe association\n\n\n- nra\n\n\n- minnesota association\n\n\n- sup international joint conference\n\n\n- ms processing\n\n\n- australia association\n\n\n- popescu democratsrepublicans\n\n\nExtracted for label: GPE\n- qatar\n\n\n- us\n\n\n- theand\n\n\n- israel\n\n\n- umap\n\n\n- us\n\n\n- testnra\n\n\n- poland\n\n\n- us\n\n\n- pdflan\n\n\n- egypt\n\n\n- onlya\n\n\n"
    }
   ],
   "source": [
    "# Extracting the main entities\n",
    "relevant_labels = [\"NORP\", \"PERSON\", \"ORG\", \"GPE\"]\n",
    "\n",
    "for relevant_label in relevant_labels:\n",
    "    print(\"Extracted for label: \" + relevant_label)\n",
    "    for entity, label in extracted_entities:\n",
    "        if label == relevant_label:\n",
    "            print(\"- \" + entity)\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">topical tweets enough effective stance detectionyounes samih kareem darwishqatar computing research \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    institutehamad bin khalifa university\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n doha \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    qatar\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n ysamihkdarwish detection entails ascertaining position target entity topic claim employs unsupervised classiﬁcation performing stance detection vocal twitter users rahave many tweets target yield high however methods perform poorly fail comapletely less vocal users may authored tweets target paper tackle stance users using \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    two\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n</mark>\n approaches ﬁrst ap proach improve userlevel stance detection representbiing tweets using contextualized embeddings captureslatent meanings words context show apfigure sample us midterm election related tweetsproach outperforms \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    two\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n</mark>\n strong baselines achieves macro fmeasure \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    eight\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n</mark>\n controversialcthat either express clear stance btopics \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    second\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n</mark>\n approach expand tweets given user using twitter timeline tweets unsupervised classiﬁcation user enoptimal results users rarely express opinionvtails clustering user users training setand may one \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    two\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n</mark>\n topically approach achieves accuracy macro flated tweets though single tweet might explicitly may lack sufﬁcient context determine user figure show \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    two\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n</mark>\n tweets pertain us midterm elections ﬁrst expresses lucidprorepublican stance \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    second\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n</mark>\n could austance detection entails identifying position supporter either \n<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    republican\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n</mark>\n democratictowards topic entity claim mohammad et paper aim effectively identify effective stance detection particularly twitter users towards speciﬁc targets entities topicsof social media instrumental gauging public opinwhere users mentioned targets fewion identifying intersecting diverging groups unvtweets less \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    two\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n</mark>\n tweets averagederstanding issues interest different user communiixto employ \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    two\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n</mark>\n approaches ﬁrst apties magdy et al much recent works explored varying stance detection methods including super proach classify user based tweets repravised semisupervised unsupervised user classiﬁcation resented using contextualized embeddings capturedarwish \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    et al magdy\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n et al pennacchiotti latent meanings words context speciﬁcally \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    usepopescu wong\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n et al much work bert embeddings represent tweets ﬁne tune thehas focused stance detection twitter users dif embeddings every topic compare approach toferent approaches advantages disadvantages \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    two\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n</mark>\n strong baselines namely using support vector machineexample supervised methods simple implement svm classiﬁcation fasttext deep learnthey require manually annotated training data accu ing based classiﬁer \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    second\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n</mark>\n approach expand theracy varies widely based classiﬁcation features clas tweets given user using twitter timeline tweetsand perform unsupervised classiﬁcation usersiﬁcation technique number training test examples magdy et al though semisupervised clustering himher users training set usunsupervised methods typically use user interactions ing expansion allows \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    us\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n make use echo chambersten may yield perfect classiﬁcation effective clas form twitter users similar views tendsifying highly vocal users many topical tweets dar retweet similar accounts beyond topic hand testwish et al methods produce sub approaches used dataset containing tweets po larizing uscentric topics also examine effect ex methods one major downsides classiﬁcationpansion using svm fasttext contextualized em methods need seed list manually labeled usersbeddings testing randomly selected users time consuming requires topic expertise sueach topic less topical tweets man pervised learning sensitive classiﬁcation featuresually labeled stance construct training set size training sets number available tweetswe used unsupervised stance detection automatically la users test set classiﬁcation algorithmbel active users per topic every topic borgeholthoefer et al common classiﬁcawe used balanced set users per stance training tion features include lexical syntactic semantics feaset darwish \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    et al\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n since approaches rely dif ture network features retweeted accounts userferent features utilize different classiﬁcation techniques mentions content features words hashtags andwe indicate approach works best different con user proﬁle information name location aldayelditions magdy magdy et al magdy darwishand weber pennacchiotti \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    popescu somethe contributions paper\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n follows commonly used classiﬁcation algorithms include \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    svms andwe ﬁnetune\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n contextualized embeddings generate ladeep learning classiﬁcation \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    zarrella marsh\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n representations tweets effectively classify thepopat et al present neural network model stancestance users based \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    one two\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n</mark>\n tweets weclassiﬁcation augmenting bert representations aachieve fmeasure arenovel consistency constraint determine stance resigniﬁcantly h igher scores achieved bothspect claim perspective extend workbaselinesin \n<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    two\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n</mark>\n ways namely drop need claim show using additional timeline tweets \n<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    theand\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n</mark>\n perspective couple \n<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    bert\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n</mark>\n supervised classiﬁcausers wish classify using unsup ertion unsupervised classiﬁcation effectively tag vocalvised classiﬁcation cluster test user \n<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n    withlabeland nonvocal users\n    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n</mark>\n semisupervised m</div></span>"
     },
     "metadata": {}
    }
   ],
   "source": [
    "# Visualizing named entities\n",
    "displacy_image = displacy.render(nlp(df['clean_text'][0][:5000]), jupyter = True, style = 'ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitkagglecondae89e86991a1240919be5d9bbaaafe61c",
   "display_name": "Python 3.7.7 64-bit ('kaggle': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}