{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install flask-ngrok==0.0.25","execution_count":5,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: flask-ngrok==0.0.25 in /opt/conda/lib/python3.6/site-packages (0.0.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from flask-ngrok==0.0.25) (2.22.0)\nRequirement already satisfied: Flask>=0.8 in /opt/conda/lib/python3.6/site-packages (from flask-ngrok==0.0.25) (1.1.1)\nRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->flask-ngrok==0.0.25) (3.0.4)\nRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->flask-ngrok==0.0.25) (1.24.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->flask-ngrok==0.0.25) (2019.11.28)\nRequirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->flask-ngrok==0.0.25) (2.8)\nRequirement already satisfied: itsdangerous>=0.24 in /opt/conda/lib/python3.6/site-packages (from Flask>=0.8->flask-ngrok==0.0.25) (1.1.0)\nRequirement already satisfied: Jinja2>=2.10.1 in /opt/conda/lib/python3.6/site-packages (from Flask>=0.8->flask-ngrok==0.0.25) (2.11.1)\nRequirement already satisfied: click>=5.1 in /opt/conda/lib/python3.6/site-packages (from Flask>=0.8->flask-ngrok==0.0.25) (7.1.1)\nRequirement already satisfied: Werkzeug>=0.15 in /opt/conda/lib/python3.6/site-packages (from Flask>=0.8->flask-ngrok==0.0.25) (1.0.0)\nRequirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.6/site-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok==0.0.25) (1.1.1)\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import torch\nimport transformers\nimport flask\nimport time\nfrom flask_ngrok import run_with_ngrok\nfrom flask import Flask\nfrom flask import request\n#from model import BERTBaseUncased\nimport functools\nimport torch.nn as nn\nimport joblib\n\n\napp = Flask(__name__)\n#run_with_ngrok(app)\n\nBERT_PATH = \"../input/bert-base-uncased\"\nMODEL_PATH = \"../input/bert-sentiment/model.bin\"\nMAX_LEN = 512\nTOKENIZER = transformers.BertTokenizer.from_pretrained(BERT_PATH, do_lower_case=True)\n\nMODEL = None\nDEVICE = \"cuda\"\nPREDICTION_DICT = dict()\nmemory = joblib.Memory(\"../kaggle/working\", verbose=1)\n\nclass BERTBaseUncased(nn.Module):\n    def __init__(self):\n        super(BERTBaseUncased, self).__init__()\n        self.bert = transformers.BertModel.from_pretrained(BERT_PATH)\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 1)\n\n    def forward(self, ids, mask, token_type_ids):\n        _, o2 = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)\n        bo = self.bert_drop(o2)\n        output = self.out(bo)\n        return output\n    \n\ndef predict_from_cache(sentence):\n    if sentence in PREDICTION_DICT:\n        return PREDICTION_DICT[sentence]\n    else:\n        result = sentence_prediction(sentence)\n        PREDICTION_DICT[sentence] = result\n        return result\n\n\n@memory.cache\ndef sentence_prediction(sentence):\n    tokenizer = TOKENIZER\n    max_len = MAX_LEN\n    review = str(sentence)\n    review = \" \".join(review.split())\n\n    inputs = tokenizer.encode_plus(\n        review,\n        None,\n        add_special_tokens=True,\n        max_length=max_len\n    )\n\n    ids = inputs[\"input_ids\"]\n    mask = inputs[\"attention_mask\"]\n    token_type_ids = inputs[\"token_type_ids\"]\n\n    padding_length = max_len - len(ids)\n    ids = ids + ([0] * padding_length)\n    mask = mask + ([0] * padding_length)\n    token_type_ids = token_type_ids + ([0] * padding_length)\n\n    ids = torch.tensor(ids, dtype=torch.long).unsqueeze(0)\n    mask = torch.tensor(mask, dtype=torch.long).unsqueeze(0)\n    token_type_ids = torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0)\n\n    ids = ids.to(DEVICE, dtype=torch.long)\n    token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)\n    mask = mask.to(DEVICE, dtype=torch.long)\n\n    outputs = MODEL(\n        ids=ids,\n        mask=mask,\n        token_type_ids=token_type_ids\n    )\n\n    outputs = torch.sigmoid(outputs).cpu().detach().numpy()\n    return outputs[0][0]\n\n\n@app.route(\"/predict\")\ndef predict():\n    sentence = request.args.get(\"sentence\")\n    start_time = time.time()\n    positive_prediction = sentence_prediction(sentence)\n    negative_prediction = 1 - positive_prediction\n    response = {}\n    response[\"response\"] = {\n        'positive': str(positive_prediction),\n        'negative': str(negative_prediction),\n        'sentence': str(sentence),\n        'time_taken': str(time.time() - start_time)\n    }\n    return flask.jsonify(response)\n\n\nif __name__ == \"__main__\":\n    MODEL = BERTBaseUncased()\n    MODEL.load_state_dict(torch.load(MODEL_PATH))\n    MODEL.to(DEVICE)\n    MODEL.eval()\n    app.run()","execution_count":6,"outputs":[{"output_type":"stream","text":" * Serving Flask app \"__main__\" (lazy loading)\n * Environment: production\n\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n\u001b[2m   Use a production WSGI server instead.\u001b[0m\n * Debug mode: off\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}